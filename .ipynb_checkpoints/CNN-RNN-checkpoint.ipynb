{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b4b92d-9297-49db-aa7a-ad2f49d62725",
   "metadata": {},
   "source": [
    "## **[Note]** In this file, you have 3 tasks to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3731cb2-0ac4-4f25-915c-54b97c5b9c58",
   "metadata": {},
   "source": [
    "In Project 1, you will learn how to use CNNs for Chinese character recognition. In Project 2, you will use RNNs for text sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67143d-24cf-4eae-ae87-e89f4d9a6dec",
   "metadata": {},
   "source": [
    "# Project 1 : Chinese Character OCR Recognition Using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa459e-b237-48ca-92cb-71c2c1782eed",
   "metadata": {},
   "source": [
    "Chinese characters, as symbols of the long history and rich culture of China, embody profound national sentiment and wisdom. From the evolution of oracle bone script to simplified characters, Chinese characters have not only been essential tools for recording history and conveying ideas but have also served as a bridge for communication and unity among various ethnic groups. In modern society, Chinese characters remain core elements in fields such as education, technology, and cultural transmission. However, with the rapid advancement of information technology, a significant amount of Chinese content still exists in paper or image formats, posing numerous challenges for organization, storage, and sharing. \n",
    "\n",
    "In this context, OCR (Optical Character Recognition) technology plays a particularly important role. OCR technology can accurately recognize Chinese characters from images or paper documents and convert them into digital text, enabling efficient retrieval, editing, and archiving of Chinese materials. This not only facilitates document management for individuals and businesses but also creates new possibilities for the digital preservation and cultural dissemination of Chinese characters. Furthermore, OCR technology is widely applied in fields such as education, archival management, and historical document restoration, aiding in the preservation and innovation of traditional culture. Therefore, in today’s information society, OCR technology is crucial for the digitization of Chinese characters, providing strong technical support for the continuation and dissemination of Chinese culture.\n",
    "\n",
    "In this assignment, we will apply the knowledge learned to create a simple Chinese OCR system. Through this project, we aim to deepen our understanding of OCR technology and explore its practical applications in the digital processing of Chinese characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24e970-a68c-4c1a-951a-06cfa5a180d4",
   "metadata": {},
   "source": [
    "## System Composition Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650ee80-b03c-4bea-b00f-9e41216e0a42",
   "metadata": {},
   "source": [
    "Our system is composed of two main components to achieve basic Chinese OCR functionality:：\n",
    "\n",
    "The first part is a Yolo model used for detecting the position of text within images. The Yolo model quickly and accurately locates text regions within an image, facilitating subsequent character recognition operations. To simplify project implementation, a pre-trained Yolo model is provided, eliminating the need for retraining; this model can directly be used to detect text locations within images.\n",
    "\n",
    "The second part involves a CNN-based character recognition model designed to identify the content of individual characters. For this section, you can either construct a custom convolutional module to build a recognition model or directly use existing convolutional architectures such as ResNet or MobileNet, known for their strong feature extraction and recognition capabilities. Using this character recognition model, we can sequentially identify the text regions located by Yolo into specific Chinese characters, achieving comprehensive text extraction from images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d866f1ac-5239-4fc6-b7af-f9d2e87cbc1e",
   "metadata": {},
   "source": [
    "## Install the relevant dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f88e49-8b79-4098-a24f-ff009fb1f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "! pip install pillow -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "! pip install torch torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "! pip install ultralytics -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "! pip install transformers -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "! pip install jieba matplotlib numpy pillow tqdm scikit-learn pandas -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6cf9c-99c2-4dfd-85d2-dcd785701f76",
   "metadata": {},
   "source": [
    "## Character Recognition Model Based on CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff4280-7c5e-4e89-ab60-0fe13c157d6c",
   "metadata": {},
   "source": [
    "### I. Text Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee2558-fb6a-4202-b337-80a296a742b7",
   "metadata": {},
   "source": [
    "The text detection model is a Yolo model used to identify the position of text within images. The Yolo model can quickly and accurately locate text regions in an image, facilitating subsequent character recognition operations. For easier project implementation, a pre-trained Yolo model is provided, eliminating the need for retraining in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13725144-a0db-440e-b560-47b25d8d326d",
   "metadata": {},
   "source": [
    "Use the code below to test the detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464c542-0d2d-458e-a5a1-6dfe685011c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "# Load a model\n",
    "yolo = YOLO(\"best.pt\")  # pretrained YOLO model\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "results = yolo(\"IMG1.png\")  # return a list of Results objects\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    plt.imshow(result.plot())  # Display the original image\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94e1f8-5283-40bd-8960-1148947a5992",
   "metadata": {},
   "source": [
    "### II. Character Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6499d48c-c77f-4008-a62d-354f202aaa08",
   "metadata": {},
   "source": [
    "#### 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fe79c-b0a5-42bb-8095-afe3c1eafbc4",
   "metadata": {},
   "source": [
    "To complete our task, we need to obtain a dataset. We can achieve this by using Python to generate characters, allowing us to produce a large number of character image files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c6356-a38c-4734-b911-06a7d13eeb9c",
   "metadata": {},
   "source": [
    "Here are all the characters we need to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb826596-eb5a-4e69-b4dc-0adc3a727a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"账树并擦最仓莱桌蔑编锐柒畦汪苞舵秦便息饿；雏央偷母葱浙忱碾衅运辰忿魏豁嗓嘲吓货叫谒摧飘拖砚养亏咱倦砾夭摘坛琳铐拟谬书逻翅箍缝例丑好历溃依掠银稿惜霞椰尽仁蛮工稳互严礼葵暑倘保酒遭仇许做痪橡丝疮件穆败愤滤幅闻翻辅隙铲弦糙代赠殷锡租仙位鲸舷符橱酷猩汗氓挚地饵安阿样泄唬憔干贩屹曼芥姜捎疾纱拴羔桂帜8插凌驼统寻阁杉丐凡验鬓瘫惩虏毕日环趟瘟边惰测卢溺住劈蜗摊祠组木恤琉引疙旦晦射萌策昆犬推q脱预晚抵奉二中违萍瘤诚藕常烟萤菲涌攘吕贯劣俱势诞水余妖限肌美漂参宋昭达吆说蕊咐臀旅挟苹控均阐殴涝祷轨摩融聪速踪驴扩谅辉形颖蝗皱北壁勤肠域妹畏社芋摄南忌贴捣蛋鳄充逛箫遥衍豌尸异嚷狸俘核糯酥篡扒沸觉震跷妻践状弄沛迫瞬漱沐伶绘氯提纪疯记始沥锦农哮详乘残栈杜技音现扼谴碌税晕先辕屈卦刺〉吉楞辨别煮撕稚镣倡生t锋度撞徘敦券唆喇菜k俺盗酸梭恼叼肛轴河允咪穿耀芳菱悉画急矫版业嘶挫莹炼桐拘怔捐润进鸭肾埂胯芯双狰彰坦霸的挤馋滚铆膨，侯+要蟋隐修栋乙倾虫怪么亮矮串腔带呛淡培霹儡李宿眯她瓮夸哟扛谁氮搅迅辞空诫雹截映滴哪肉诲陈俐易耗茁搀械砖容泌泡癌兽心剂首岛灰憋桶添澜把巾斗壳众卓益莉）传抹葫遂防译杂侧烛煤岁哑煞揭言分给向岖痘颗谚咒宠宅筒殃雀锉箩荒宛畔壹醇睦筐源蹭坞百但薪轿击喝厂姥牍花渡赴显酵诊泳贱络恐杠梅盟铃擎苦旷《半窒捡漠奢侵牺棠挣己丙刊混确胡准腾毫悔云蔬枯师远钞搭史早量恕祈英鸦呼家敢武乍腮坟陷悬思秃爬楔瞻米叮殖皇坏浆狈濒多锈享窝剃甫畜汁腰器饰拄殊搏川宽蓄撮公轩钉侣纽屏竣爸淌鸳摇崖蛔蟥敏绎缀逗扶茅乳肢蕴彻般节晶恬民p秩瞭巡志果系婴惹簇恶绒操茂览追吁召疑流袋直包肿玫味幽垒昨构超古呀竞顾赚钓咕e欠凹绕咏探腿覆波抒未骄狼性破乔鞋忆听竿掸普顿者句立兑榕将碱脆钝哥定拗冰韵璧淫挺兴讨查同窜秽如利敛叙跋虱汛蝇种捧莺纫歉卵权矩缰境缨睬展仲肯悼套浑铅评妈谍致傀妒跳阵拣糖兰陆疗床晰髓膝妙具市毯赁驾痴伤皿眨踊模委较涂暂攻等制喧触肖畴粹抗钻灿创崇抚批憨助赵拌卜期哲深孕貌豺哀谆番谋执搬淤响情待嗤马叶站烦茬什训洞庞廉窖歼谜蔽寨患盔肘阎幌遵弹翁名涮斋外涤掰刮椅昔晾采剑檬赋篱拾怖高励a率裸慎乓搞卑奠暮议媚末野括诽币乏布智佣邻紊彤长嘉粗葡酣欺虎锤衫数狞绩龄拱国倒得孽盘张讥回寝食饺刁艘炸秸寺其匈啦咬媳\\\\觅蘸辈惨储痊企躬蜒我胰缔士晃傅恒溢镶循淮针贷囱妨糊醉偎颂亲满惋冻盛所剧盐共蟀仿址寇撼石帝选前臣倚俏置挨雪俭笆淑转宫曲淋噪喘意侮罚啸遮〈蛛奖嚼良翩宏绳挥硬鞭卷搪惫累弛巨锻各怕咳坎昙烘窿死怒陌姿朽诅行褪烹了夺旨斧禀蚀z罐不诉印蝶爆单拯嚣焚黎徒鹊启雕次栖}奥诈蓖泥卡庶佩姨绣着蛾万慌赔嗦法j吞取氏肺庄哗闸宴放谢轰案供卧漫椭冤碎盖蚪缩欣甜堵顷蔚蚕炒皮肋爷骗戚每能喻请扎王潮护齐汞株颈勾阶亥售缅摸仆倔猜扬麦富羹资蓬浩丢你敌躯族营沧借辑身锄舟锹萎刻灸旁怎捌酱伦燃暗愧鸵垮匪6悯熄拂捺粪烧迈盏搂窃输上堤撤答畅昏懦之帽珍终溶吝朱光缕涧缺蓝陡投唱契蟆窑湿至词冀程春类员邀油猫阻浪笔持随本厌乎加车芦迷锯课冠升嫁栏质拦消瀑舱垫症琢榛橙叉饮郑冷蝎作算仰尖买幸娜披琅逸瑟辜知个优坯软g糜隘洋旧霎相摔竹h蝙讯饭愕谓蹋魔b矛斯滨凯阴约损蒸述压菊筋惑楼码片徽潭讹割台停胀务鳍菠馅九曙路哨誓式巴略棒履田敲匙播拳琴毡朋失坡州煌按吮染扯裕实辐邢餐啥岗闹题赏嗡梧抑-计瞎洽胚鉴耽皆填唇彪负柜救讳火澎i愉锣用植抽忙忧下默呕笑温夫蝉艇兢础槐胎熙几与“闰罪腥毛求蛉气尼鸥篇艺寄阅亿欲凰扔它螺娘铡松玻井泽峰淹揍唾适薄京有拼硫您邑跛扣秋唯诬协吱骂罩欧足晤辫纳教丹密塑巍冲亩朗缴脐婿县豪糕再扭鸠絮往炬照退瞄尚复s凸另役喳似念褥鹃墅点踏完绸跨蛙欢缸毅悲蜘鞍扁森雁掂起柬碟庆埋可夏铁维股拷舆禁秉刃池笤硝香滩沈林杏辖押仪科沫撇娄垂演垦栗吸俄肴鹏链牧傻峦豹浮鲜杖吟忠登袱馁鞠吊浸过尊步泼猬蝌剖伊阀躁臊咸圈旱梁然玩蜀佳金善反荠惠锭薇块拐端焕是联赤坠挎刹孵狱抖永配烁鸿呈嘱芝鸯遗贬霉掘祸也况内垄捂猎时镐瘩激殉谐土蚓字逮拨警柱笙明‘型凝趾溅漆喜角箭腌黄耘世袄削藏交吠财飞矾茸劲剔贼奋跪茵于蚯膀馒肮束纺堂久椎甚绪胸迹碳经褒受躺隅逃脏镇攀篙狭邦月陶需尝弥专炮惭争厅沮颊膊抓菌掷替慰靴里招象原秧d岳渊f贫衬造刚橄勇自#慢聚狐唠溉临都手孔继弱猛讶吴热练俯告宗血岭慕那梗炕脑恳泛瓜幕含湖繁董间喉廓氢开年揖渣兜苟表浦砌癣童磷声产玄椿隔嫌揩攒隆坐簿胧寞闺装僻烂屁蜈真蜻涡鲫骤娩疼赐逼鸟蚂钾篷颁予些湃侨跟喷郭屑讼捷梳卸柴尤栽撬熟捻沦闽抡圆称旋渔被汇估茧乒右糠儒曹框叹剪示避除徙面屡寓司檩鳞蹄窥吗惶学派频铣虑总虾兄责呜免倍勿七瓢险递叔恍十斟雇叨晋铺条鹅灌越衷换揽牛楷吐申天迁唉三队谈彬贺枉栅丁蜓喂禽泪贮债轻力戏征熏全伪辣镰铜懒脊炭愚锌螃舅淀去戴伍猴及号耻呆职牡沉宜颜份鹤革肩惯纯穗凤斥究镊歇汰贝薯朦露耐茫滓浓2雾铝接座秀库禾蜡榆颠吵灵拢焙昌蒋物翼袒鹉才没侥入男枣丰西精衔奔津沪医陪掉痰」n四宁肚雨胁憎躏躲额差梨膘棱闲槽毙沃惦茶文锰打函盅圾注杭%勃撩缓祭鲤猖佑塞邓柔掏潜梯绑姓碰旬赦蚊目恨翎补妓筑少鼓话导坑匕肪襟嘀痛球匠夷奶薛戳艰狡仅成捅庙瘦俗须涵考腕叭逞蔓宰军备萄闪宝姑圣裤疟{见x掌问希恭道贾介判乐收捍谷囤钦醒誊蜂唁捏捉沙宾假涩支吩封y隧缭党盹亦忽榔认青服掀苛移吭俩潦厉局初街磕纬误捞蒲紧散梦管庭铭苗拥帘友斤仗凛穷荷妆墩盼材勋键绊壕癞织山骆耕婶霍帖耍访简蛤奇腺突湘鸡r裆托骏堡段9牵匀否叛脸秫崔坷尔洼蚜翰口蚌昵蹂擅藤辛弓朝愈郊塌l到：坪1刑姻振伏料誉衣曾窘豫垃黍连仍尾赛鹿惕龙剥枝谤落饥屉拧骑辽跺范款婆洒靶稀感岂脖群撵游渴焦顶醋腐、庐整兼崎桑哼卫止嘹幔机熊绿闷官忘窍基绅灶呻一特踢宵两贞祝格哺焰幻=雅胳狮舍胖馍吃诱届摆撑尘厦旭邮婉萧羊虹图门缚嘴芹燥以建滞钳宪洪鸣牌厘俊颤粘涨人毁姐部尿袖趋衡墨署吻廊绽甥迟签战僚赃囊厚风班杆怠辱3积户菇芙途贤洗雄棉胞玷笋藐肆掖！怯箕滑正挖蒙购赫隶污城轧快新拭奄舰膳匣办秆白喊兔敷荣灾检关背弟今讽凳拇嘿碉剩增毒御他酗姊静规液鹰删钙来为钧校峻脯凑钟炉蹬抱慧更妇区监奕伙吹鹦赌们揣兵底帮玉爪奏拜呢错磁素靖熬劝笼虽乾授柑私延纹罢送甘榜折改绷销妄漾峭慨唤裳盲歌命诗憾蚤偏苫砂票稼陋舀腊耸奴凶粤逆疏鱼啰承横怀幢舌贵琼檀察沼枷歹谊魄v锚蛀调赊瓣爵剿头而诀怨梆费坊缎晒粟夕当缆冗堕澳千驻船莽爱遣扑店帆报信脓痒害必棋无侄褂珊斜尉耿壮逝会悟绢桅哈凫萨习列叁聋劳啤碴汽弃蠢犀迄非蕉诸矢哩兆牢洛绰比荸樱杈厨4疚钢踱女蜜挪孝园帅付影冶粒勉父还哆勺只奸太孙渠瞧徐泣写抄肝悄嗅因耳o檐乱帕指烫沟岸厢恢傲呵蚣拉榴窗帐呐佃匿绍附瓤蒿就飒订熔驮罗刨功审睛揉层羞姚汉鼻苔赖炫饶滥泰砰赂疤叽伞稽哄淘w舔蓉爽炊贸靠孟效册获谱康驹侠暇u咽墙裂揪跑蝴裁腻丧决眉伸铛蒜草啡药像镜副惧政伴冯悦螟老虐守柏障缠绝祖擒杯周疹距贰纵走扮勒集麸谭艾韭重枢芍伟低棵眶愿嫉邪漓锁姆璃舶磅搁驳？让逊樟嗜籍崩变夹捶骚帚巢蚁透遍漩态聂候催神撒褐危活庸丽娇疫广掩威斑腋这矿芜钠咨袜挑很领侈元咆蛇赡讲澄酪析疲级衙靡滔嫩标废韧梢痕垛搜柿困殿狗寂黑主甲逢沾盯瞪体寒卤锥跌综柄典雷咖卿硼平幼挂泉。荐昼谎左弧颇渐僵轮啃缤拙踩彭东通胜筏譬占榄切冈芒在闯舒眠肄汤仑馆握由饲聘昂猪粱悴德饼骇钱戒竖断赘纷刷悍降牙驰理胆猿凭发楣客郁蠕猾净询亭瓦寥祥堰碘线近湾航塔澈晓枪桃乖荧m叠脉蹦鄙碧偶钥挽却价羡逐篓塘弊视泞玛潘狠嘁冕使独茄挠商妥秕席纤嗽甸极措凿钩担岔怜翠茎细且举撰舞小根懈晌努爹抛豆坝克架任葬港辙瘸皂搔玲腹围哎陕趣衩棘鼎丘敬溪抠档存”绞杨吼孤暖栓纸冒棚陨搓漏稻&柳牲昧解清袭嫡酌糟筹红筷耙簸室固盒罕涣噩茉眷府膜砍谨啄钮督懂翘旺值项灼苇畸蔼辩渗烤菩碗电纠痢后冬霜盈*犁难孩荞侦氧骨续蟹荡施藻则卖焊烈棕眼瑰矗语摹臭诺观既狂酿陵吨唧僧娶归酝擂驱网芭该八溯返恰减趴够休瓷闭泻佛涉胶鼠涯伯甩瑞某桩疆稍拆惊诵苍结品巷逾博翔概慈迎歪斩袁玖君即此瘾崭洁泊乃刽色芬岩铸找若彼睡埃涎描粥旗录沿留属淆济赎泵遇壶荆丸扰弯慷浴扫咙刘抢晴劫拍援抬睹令掺膏氨朴恋筝坤贡@榨子哭碑蛆锨挡硅愁虚恃强裹庇雳炎屿樊羽巧场著凉乡脚橘艳伺偿燕房蔗诡寸序宣乌蛹界磨驯拒赶晨吧阳巫窄苏笨扇五楚滋郎捆秘龟秤识桥病福离箱动戈柠棺齿秒枕想笛嵌燎奈犹渤肥捕望亚堆团啊c嚎合献馏对缘纲臂顽胃桦试微荔浊蒂拓研索零（棍涛氛拔遏珠凄粉脂海顺窟匆仔季礁悠魁驶脾茴篮祟趁携丈迂阔烙镀鲁盆赢麻阱埠庵枫乞萝屯荚术瞒廷衰蹲袍筛第宦杰莫华板鳖星裙涕5壤杀已或短肤勘敞澡尺据竭儿页酬荤堪砸婚丛读淳浇》夜义硕溜夯际坚巩村忍寿方匾】颓午贿治臼痹择锅大犯应谦排椒化侍贪囚0朵划故蘑墓蹈院桨载枚圃芽碍从睁省江伐彩【盾设亡瓶景居掐琐六魂促卒蔫穴暴论厕健「媒鬼辆紫娃饱屎宇刀看垢跃何募膛灭蜕瘪浅和瞳证灯懊渺谣汹又径骡唐蝠寡律辟章嫂籽屋育啼事颅粮7蕾雌出处莲恩肃韩峡葛煎吏徊拿绵嬉释傍黔鸽屠沽咧娱竟歧匹稠扳洲赞宙聊QWERTYUIOPASDFGHJKLZXCVBNM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0482e6e5-4d2b-4bba-8c83-147a4d30cdd2",
   "metadata": {},
   "source": [
    "We can use the len method to get the character count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b40140-72e8-4b80-b4fa-d10f573707c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of labels is :\",len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ed17b-9b77-456a-9e19-f00da7ddf9fc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72baa768-a07d-48ae-8aaa-5a03c2070bbf",
   "metadata": {},
   "source": [
    "We use the following code to generate samples.\n",
    "\n",
    "**[Note]** You will need approximately 6GB of space to store these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b479966-e645-41cf-8259-24c6f2a1e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Process, Value, current_process, Lock\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "# Define a function to generate a sample image with a character using a given font\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "def generate_sample(char, font_path):\n",
    "    image_size = (64, 64)  # Set the image size to 64x64 pixels\n",
    "    background_color = (255, 255, 255)  # White background color\n",
    "    \n",
    "    # Create a new image with the specified size and background color\n",
    "    image = Image.new('RGB', image_size, background_color)\n",
    "    draw = ImageDraw.Draw(image)  # Create a drawing object for the image\n",
    "\n",
    "    # Dynamically adjust the font size to fit the character within the image\n",
    "    max_font_size = 48\n",
    "    min_font_size = 10\n",
    "    font_size = max_font_size\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    \n",
    "    # Gradually reduce the font size until the text fits within the image\n",
    "    while font_size >= min_font_size:\n",
    "        bbox = draw.textbbox((0, 0), char, font=font)  # Get the bounding box of the text\n",
    "        text_width = bbox[2] - bbox[0]  # Calculate text width\n",
    "        text_height = bbox[3] - bbox[1]  # Calculate text height\n",
    "        \n",
    "        if text_width <= image_size[0] and text_height <= image_size[1]:  # Check if text fits within image\n",
    "            break\n",
    "        font_size -= 1  # Decrease font size\n",
    "        font = ImageFont.truetype(font_path, font_size)  # Update the font with the new size\n",
    "\n",
    "    # Check if the final font size fits, if not return an empty image to avoid errors\n",
    "    if font_size < min_font_size:\n",
    "        print(\"Character cannot fit into the image size\")\n",
    "        return image  # Return a blank image if the character cannot fit\n",
    "\n",
    "    # Calculate the centered position for the text\n",
    "    position = ((image_size[0] - text_width) // 2, (image_size[1] - text_height) // 2)\n",
    "    draw.text(position, char, fill=(0, 0, 0), font=font)  # Draw the character on the image\n",
    "\n",
    "    # Convert the image to a NumPy array and add random noise\n",
    "    image_np = np.array(image)\n",
    "    noise_level = 10  # Set noise level\n",
    "    noise = np.random.randint(-noise_level, noise_level, image_np.shape, dtype='int16')  # Generate noise\n",
    "    noisy_image_np = np.clip(image_np + noise, 0, 255).astype('uint8')  # Add noise to the image and clip the values\n",
    "\n",
    "    # Randomly apply erosion or dilation operation to the image\n",
    "    kernel = np.ones((3, 3), np.uint8)  # Define a 3x3 kernel for morphology operations\n",
    "    processed_image_np = noisy_image_np\n",
    "    if random.random() < 0.5:  # Randomly decide whether to apply erosion\n",
    "        processed_image_np = cv2.erode(noisy_image_np, kernel, iterations=1)\n",
    "    \n",
    "    # Convert the NumPy array back to an image\n",
    "    processed_image = Image.fromarray(processed_image_np)\n",
    "    if char not in '._＿':  # If character is not one of these, apply random rotation\n",
    "        rotation_angle = random.uniform(-5, 5)  # Random rotation angle between -5 and 5 degrees\n",
    "        processed_image = processed_image.rotate(rotation_angle, expand=False, fillcolor=background_color)  # Rotate image\n",
    "\n",
    "    return processed_image  # Return the final processed image\n",
    "\n",
    "# Function to determine if an image is blank (using grayscale variance method)\n",
    "def is_blank_image(image, threshold=50):\n",
    "    gray_image = np.array(image.convert('L'))  # Convert image to grayscale\n",
    "    variance = gray_image.var()  # Calculate variance of the grayscale image\n",
    "    return variance < threshold  # Return True if variance is below the threshold (indicating blank image)\n",
    "\n",
    "# Define function to save generated sample image\n",
    "def save_sample(image, label):\n",
    "    folder_name = \"./generate_data/\" + label  # Define folder path for the label\n",
    "    os.makedirs(folder_name, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "    file_count = len(os.listdir(folder_name))  # Count number of existing files in the folder\n",
    "    file_name = f\"{file_count}.png\"  # Define the new file name\n",
    "    file_path = os.path.join(folder_name, file_name)  # Get the complete file path\n",
    "    image.save(file_path)  # Save the image to the specified path\n",
    "    \n",
    "# Function to check if a character is supported by the font\n",
    "def is_char_supported(char, font_path, font_size=48):\n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, font_size)  # Load the font with the given size\n",
    "        # Attempt to get character bounding box to check if the font supports the character\n",
    "        if font.getbbox(char):\n",
    "            return True  # If bounding box is found, character is supported\n",
    "    except OSError:\n",
    "        pass  # Handle font loading error\n",
    "    return False  # Return False if the character is not supported\n",
    "\n",
    "# Function to process a subset of labels\n",
    "def process_labels(labels, font_files, count, progress, lock):\n",
    "    for char in labels:\n",
    "        font_paths = random.choices(font_files, k=count)  # Randomly choose fonts from the font files\n",
    "        for font_path in font_paths:\n",
    "            # Check if the font supports the current character\n",
    "            if not is_char_supported(char, font_path):\n",
    "                print(f\"Font {font_path} does NOT support character '{char}'\")\n",
    "                continue  # Skip unsupported fonts\n",
    "\n",
    "            tryCnt = 0  # Initialize retry counter\n",
    "            image = generate_sample(char, font_path)  # Generate a sample image\n",
    "            while is_blank_image(image) and tryCnt <= 10:  # Retry if image is blank\n",
    "                tryCnt += 1\n",
    "                # Randomly select a new font that supports the character\n",
    "                newfont = random.choice([f for f in font_files if is_char_supported(char, f)])\n",
    "                image = generate_sample(char, newfont)  # Generate image with the new font\n",
    "            if tryCnt > 9:  # Stop retrying if limit exceeded\n",
    "                continue\n",
    "            save_sample(image, char)  # Save the generated image\n",
    "            \n",
    "            with lock:  # Lock to update shared progress safely in multiprocessing\n",
    "                progress.value += 1  # Increment progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100160a9-329d-4599-85e4-4bfd754cfb02",
   "metadata": {},
   "source": [
    "We use multiprocessing for generation; it may take you half an hour to an hour to generate the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f59bba-621e-463d-b625-9454395b820e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40770d96-d7c6-4af2-b965-b979a56168f8",
   "metadata": {},
   "source": [
    "## Task 1 (10 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a5351-9c88-4111-a70e-9f540dab1b68",
   "metadata": {},
   "source": [
    "**Problem Description**:\n",
    "\n",
    "In this task, you will implement a multi-process dataset generator. We have provided you with the `process_labels` function, which generates samples based on labels and font files. Follow the requirements and hints below to complete the code.\n",
    "\n",
    "**Task Requirements**:\n",
    "\n",
    "1. **Font File Reading**(1 Pt): \n",
    "   - Define the variable `font_directory` as the font file storage path `./Font`. Read all `.ttf` and `.ttc` files from this directory and store their paths in the `font_files` list.\n",
    "\n",
    "2. **Calculate Total Sample Count**(1 Pt):\n",
    "   - Set the variable `count` to specify the number of samples to generate per label, with a generation quantity of 100 samples per label.\n",
    "   - The `labels` list, which contains all labels, has been previously defined. Calculate the total number of samples, `total_samples`.\n",
    "\n",
    "3. **Shared Progress Counter and Lock**(2 Pts):\n",
    "   - Create a multi-process shared variable `progress` (using `Value`) to track the total number of generated samples.\n",
    "   - Create a `Lock` object to manage access to shared resources.\n",
    "\n",
    "4. **Label Splitting**(1 Pt):\n",
    "   - Define the variable `num_processes` to indicate the number of processes. Set it to the number of cores on your processor.\n",
    "   - Split `labels` into `num_processes` chunks so that each process handles one chunk of labels.\n",
    "\n",
    "5. **Multi-process Implementation**(2 Pts):\n",
    "   - Create a `Process` instance for each chunk of labels and call the `process_labels` function to handle that chunk. Pass `font_files`, `count`, `progress`, and `lock` as arguments to `process_labels`.\n",
    "   - Start each process so they run in parallel.\n",
    "\n",
    "6. **Progress Bar Implementation**(1 Pt):\n",
    "   - Use `tqdm` to display the total progress of sample generation, updating once per second.\n",
    "   - In a loop, check the status of each process and use the `progress` variable to update the `tqdm` progress bar until all processes finish.\n",
    "\n",
    "7. **Wait for Processes to Finish**(1 Pt):\n",
    "   - Use `join` to ensure that all processes complete before the code continues executing.\n",
    "\n",
    "8. **Completion Message**(1 Pt):\n",
    "   - After all processes are complete, print “All samples generated”.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- Ensure that you pass the correct parameters to `process_labels` in each process.\n",
    "- Use `lock` to ensure thread-safe progress updates.\n",
    "- Try debugging and running the code to ensure that the progress bar and parallel processes display and execute correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd23b9fc-353a-4638-a4da-990beccd4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- Complete your answer here ---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d8512a-59f3-4028-b027-ae0d637f0fe8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd5773-1b08-44e1-8efd-b82e33add202",
   "metadata": {},
   "source": [
    "We use the following code to view the generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c46882-dc20-4603-847f-bdd7b2732ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Define a function to generate an overview image\n",
    "def create_random_overview_image(data_dir, grid_size=(10, 10), image_size=(64, 64)):\n",
    "    # Create a blank canvas\n",
    "    canvas_size = (grid_size[0] * image_size[0], grid_size[1] * image_size[1])\n",
    "    overview_image = Image.new('RGB', canvas_size, (255, 255, 255))\n",
    "\n",
    "    # Get all character folders\n",
    "    labels = os.listdir(data_dir)\n",
    "    random.shuffle(labels)  # Randomly shuffle the character order\n",
    "    label_count = 0\n",
    "\n",
    "    for row in range(grid_size[0]):\n",
    "        for col in range(grid_size[1]):\n",
    "            if label_count < len(labels):\n",
    "                # Randomly select an image file from the current character folder\n",
    "                label = labels[label_count]\n",
    "                label_path = os.path.join(data_dir, label)\n",
    "                images = os.listdir(label_path)\n",
    "                if images:\n",
    "                    image_path = os.path.join(label_path, random.choice(images))\n",
    "                    image = Image.open(image_path).resize(image_size)\n",
    "                    # Paste the image at the corresponding position on the canvas\n",
    "                    overview_image.paste(image, (col * image_size[0], row * image_size[1]))\n",
    "                    label_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Display the result\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(overview_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "data_dir = \"./generate_data\"  # Path to the folder containing generated images\n",
    "create_random_overview_image(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3178e-23ef-4ca8-9a7d-63938e6d32af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a80e8e-b36a-44a5-80a9-1a685c5e9bcf",
   "metadata": {},
   "source": [
    "#### 2. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846696ed-a4f3-411a-890d-7cb3f4b8bb6c",
   "metadata": {},
   "source": [
    "## Task 2 (40 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c7557-f2e3-4952-ae13-1437981c7a82",
   "metadata": {},
   "source": [
    "**Task Requirements**:\n",
    "\n",
    "1. **Load and Split Dataset** (3 Pts)  \n",
    "   - Use the `ImageFolder` class to load the image dataset located in \"./generate_data\", and split it into training and validation sets in an 8:2 ratio by indexing.\n",
    "\n",
    "2. **Data Augmentation and Preprocessing** (4 Pts)  \n",
    "   - Define data augmentation and preprocessing steps with `transforms.Compose`, including resizing images, converting them to tensors, and normalizing.\n",
    "\n",
    "3. **Define Model Architecture** (8 Pts)  \n",
    "   - Build the convolutional neural network model architecture in the `Model` class, designing convolutional layers, pooling layers, and fully connected layers suitable for image classification. Ensure the output layer’s dimension matches the number of classes in the dataset. You can use some ready-made convolutional frameworks, such as ResNet and VGG, or define your own model architecture.\n",
    "\n",
    "4. **Set Loss Function and Optimizer** (3 Pts)  \n",
    "   - Use `CrossEntropyLoss` as the loss function and apply the Adam optimizer to optimize model parameters with the specified learning rate.\n",
    "\n",
    "5. **Training Loop** (5 Pts)  \n",
    "   - Set the number of training epochs, put the model in training mode, and for each batch, perform forward propagation, calculate loss, apply backpropagation, and update parameters. Display a progress bar with the current average loss.\n",
    "\n",
    "6. **Validation Loop** (3 Pts)  \n",
    "   - At the end of each training epoch, enter the validation phase. Switch the model to evaluation mode, calculate validation loss and accuracy by performing forward propagation and accuracy calculation.\n",
    "\n",
    "7. **Output Training Results** (2 Pts)  \n",
    "   - At the end of each epoch, print the training loss, validation loss, and validation accuracy to monitor the model's performance.\n",
    "\n",
    "8. **Save Model** (2 Marks)  \n",
    "   - After training is complete, save the trained model to a file for later use or further adjustments.\n",
    "\n",
    "9. **Evaluate Accuracy and Optimize Hyperparameters** (10 Pts)  \n",
    "   - After completing the model training, make every effort to improve the model's accuracy by tuning hyperparameters. Your score will be awarded based on the achieved accuracy according to the following criteria:\n",
    "     - [60%,80%) accuracy: 5 points\n",
    "     - [80%,90%) accuracy: 7 points\n",
    "     - [90%,95%) accuracy: 8 points\n",
    "     - [95%,99%) accuracy: 9 points\n",
    "     -  99%+ accuracy: 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7eb1b2-410e-4e4a-a3f8-2e852f2f0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- Complete your answer here ---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151ae0f-8a20-46d6-9106-b0c1a0356f86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f31c8-1f3b-4f34-bae1-c2a982bdc053",
   "metadata": {},
   "source": [
    "### III. Fusion of Two Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8388851-4758-4ccb-a1fe-e4e40e4705db",
   "metadata": {},
   "source": [
    "You can use the code below to test the result of merging the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92119291-7bea-4328-97fb-4e1d33d2a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a YOLO model\n",
    "yolo = YOLO(\"best.pt\")  # pretrained YOLO model\n",
    "image_path = [\"IMG1.png\",\"IMG2.png\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(\"model_name.pth\")\n",
    "# Preprocessing function to align with the ResNet model input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to match the expected input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize\n",
    "])\n",
    "\n",
    "# Run inference and get the first result\n",
    "results = yolo(image_path)\n",
    "\n",
    "for index,result in enumerate(results):\n",
    "    plt.imshow(result.plot())  # Display the original image with detections\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract detection boxes\n",
    "    boxes = []\n",
    "    for box in result.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        boxes.append(((x1, y1, x2, y2), box))\n",
    "    \n",
    "    # Sort boxes by their vertical position, grouping nearby boxes into lines\n",
    "    line_threshold = 10  # Adjust this value based on text line spacing\n",
    "    sorted_boxes = sorted(boxes, key=lambda b: (b[0][1], b[0][0]))\n",
    "    \n",
    "    # Group boxes by lines\n",
    "    lines = []\n",
    "    current_line = [sorted_boxes[0]]\n",
    "    for i in range(1, len(sorted_boxes)):\n",
    "        y_diff = abs(sorted_boxes[i][0][1] - current_line[-1][0][1])\n",
    "        if y_diff < line_threshold:\n",
    "            current_line.append(sorted_boxes[i])\n",
    "        else:\n",
    "            # Sort the current line horizontally\n",
    "            lines.append(sorted(current_line, key=lambda b: b[0][0]))\n",
    "            current_line = [sorted_boxes[i]]\n",
    "    # Append the last line\n",
    "    if current_line:\n",
    "        lines.append(sorted(current_line, key=lambda b: b[0][0]))\n",
    "    \n",
    "    # Iterate over each line of detected boxes\n",
    "    recognized_text = \"\"\n",
    "    for line in lines:\n",
    "        for box in line:\n",
    "            x1, y1, x2, y2 = box[0]\n",
    "            image = Image.open(image_path[index]).convert(\"RGB\")\n",
    "    \n",
    "            # Crop the original image to get the detected region\n",
    "            cropped_img = image.crop((x1, y1, x2, y2))\n",
    "    \n",
    "            # Calculate padding (20% of width and height)\n",
    "            width, height = cropped_img.size\n",
    "            padding_w, padding_h = int(width * 0.2), int(height * 0.2)\n",
    "    \n",
    "            # Add padding to the image\n",
    "            padded_img = ImageOps.expand(cropped_img, border=(padding_w, padding_h), fill=(255, 255, 255))\n",
    "    \n",
    "            # Transform the padded image for ResNet input\n",
    "            input_tensor = transform(padded_img).unsqueeze(0).to(device)\n",
    "    \n",
    "            # Character recognition with ResNet model\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                predicted_class = output.argmax(1).item()\n",
    "                recognized_char = dataset.classes[predicted_class]\n",
    "                recognized_text += recognized_char\n",
    "        recognized_text += \"\\n\"  # Add space between lines for readability\n",
    "    \n",
    "    # Output the recognized text\n",
    "    print(f\"Result :\\n {recognized_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9edd6-83a7-4e33-9e67-5fbbea3b838b",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a87c96-68f0-480b-86ce-ce31aae597df",
   "metadata": {},
   "source": [
    "# Project 2 :RNN Text Sentiment Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56202fbe-62a2-4cc3-858a-efd820d5f5ba",
   "metadata": {},
   "source": [
    "The Recurrent Neural Network (RNN) is a deep learning model well-suited for processing sequential data and performs exceptionally in text sentiment recognition tasks. Its recurrent structure allows information to flow through the network, enabling it to effectively capture the relationships between preceding and following words in a sentence, as well as contextual information—ideal for handling the sequential nature of natural language. RNNs process text word by word, updating the hidden layer’s state to retain previous content, which influences subsequent words and helps the model understand the overall sentiment of the sentence.\n",
    "\n",
    "In this assignment, we will complete the development and evaluation of an RNN-based text sentiment recognition model. First, we will prepare for model input by performing tokenization and vectorization. Then, we’ll build a sentiment recognition model using RNN to allow the model to learn sentiment features from sequential data inputs. Next, we’ll train the model and optimize hyperparameters to enhance its performance on the test set.\n",
    "\n",
    "Let’s start by importing the necessary libraries and checking for available devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176cf8f-321e-4070-8aee-34f555860959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import jieba  \n",
    "import pandas as pd\n",
    "# Check for available CUDA device(s)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c39a7-3dbb-4ffe-a201-899491fcff8e",
   "metadata": {},
   "source": [
    "You need to set some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5608ab-f58e-495a-aa8e-1f3dbc8d6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520b5c3-12ad-410a-82c5-8dec6d10ce7a",
   "metadata": {},
   "source": [
    "We also need to load and define the dataset class, then load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cc595-4e1e-41f3-bfd6-f75a342dc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset example, assuming a format similar to the previous IMDB format\n",
    "class ChineseDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # Initialize an empty list to store data\n",
    "        self.data = []\n",
    "\n",
    "        # Open the first file and read line by line\n",
    "        with open(file_path[0], \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                # Split each line by tab to separate label and text\n",
    "                label, text = line.strip().split('\\t')\n",
    "                \n",
    "                # Skip header row if it exists\n",
    "                if label == 'label':\n",
    "                    continue\n",
    "                # Append (label, text) tuple to the data list\n",
    "                self.data.append((int(label), text))\n",
    "\n",
    "        # If there’s a second file, load it as well\n",
    "        if len(file_path) > 1:\n",
    "            data = pd.read_csv(file_path[1], skiprows=1, sep=',', names=['label', 'review'], encoding='utf-8')\n",
    "\n",
    "            # Iterate over label and review columns to add to data list\n",
    "            for label, text in zip(data['label'].tolist(), data['review'].tolist()):\n",
    "                if label == 'label':\n",
    "                    continue\n",
    "                self.data.append((int(label), text))\n",
    "\n",
    "    # Return the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Return the item at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Load the data\n",
    "train_data = ChineseDataset([\"train.tsv\", \"weibo_senti_100k.csv\"])  # Assuming there’s a train file\n",
    "test_data = ChineseDataset([\"test.tsv\"])  # Assuming there’s a test file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757c73a-d765-47dd-ae8b-a56a3d5904b2",
   "metadata": {},
   "source": [
    "We need to build the Jieba tokenizer and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fe5d0-bc5d-4622-9347-a1afb47c2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and vocabulary\n",
    "def chinese_tokenizer(text):\n",
    "    # Use jieba to split Chinese text into tokens\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(dataset):\n",
    "    # Initialize an empty set for unique words\n",
    "    vocab = set()\n",
    "    # Iterate through the dataset to tokenize and add words to the vocabulary\n",
    "    for _, text in dataset:\n",
    "        vocab.update(chinese_tokenizer(text))\n",
    "    # Assign a unique index to each word, starting from 1\n",
    "    vocab = {word: i+1 for i, word in enumerate(vocab)}\n",
    "    # Set <unk> token for unknown words with index 0\n",
    "    vocab[\"<unk>\"] = 0\n",
    "    return vocab\n",
    "\n",
    "# Build the vocabulary from the training data\n",
    "vocab = build_vocab(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2995f46-3409-42cb-810a-70258b4ed475",
   "metadata": {},
   "source": [
    "We need to define the `text_pipeline` function to convert input Chinese text into a sequence of corresponding vocabulary indices, enabling the model to recognize and process it. Next, we define the `label_pipeline` function to simplify label processing by returning labels as they are. Then, using the `collate_batch` function, we organize each batch of data, packaging text and labels into a format suitable for model input and padding texts of different lengths to ensure consistency. These processing steps provide a standardized data format for subsequent data loading and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ee0b6-ad3d-4ed8-b056-ac8a5f63f593",
   "metadata": {},
   "source": [
    "Create `train_dataloader` and `test_dataloader` using `DataLoader` to handle batch loading for training and testing data, respectively. `DataLoader` uses the `batch_size` parameter to control the size of each batch, applies the `collate_batch` function to ensure data format consistency across batches, and uses `shuffle=True` to shuffle the training data, enhancing the model’s generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79fc896-8590-4104-a0e8-f31eef95d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing function\n",
    "def text_pipeline(text):\n",
    "    # Convert text into a list of token IDs, using <unk> for unknown tokens\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in chinese_tokenizer(text)]\n",
    "\n",
    "# Label processing function\n",
    "def label_pipeline(label):\n",
    "    # Return the label as is\n",
    "    return label\n",
    "\n",
    "# Data loader collate function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for label, text in batch:\n",
    "        # Process and append each label using label_pipeline\n",
    "        label_list.append(label_pipeline(label))\n",
    "        # Process each text by converting it into a tensor of token IDs\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        # Append the processed text and its length to the lists\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(len(processed_text))\n",
    "    # Convert the list of labels into a tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
    "    # Pad the sequence of text tensors to have equal length for batch processing\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    # Return text, label, and lengths tensors, moving them to the specified device\n",
    "    return text_list.to(device), label_list.to(device), torch.tensor(lengths)\n",
    "\n",
    "# Create data loaders for training and testing data\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c0d50-0f7d-462e-996d-84e59cd68a2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294f586-fbef-4b2b-a31f-3bee432d3c10",
   "metadata": {},
   "source": [
    "## Task 3 （30 Pts）\n",
    "**Problem Description**:In this assignment, we will develop, train, and evaluate a Recurrent Neural Network (RNN)-based model for a text sentiment recognition task. The process will include defining the model architecture, training it on sequential text data, and tracking the model’s performance.\n",
    "\n",
    "**Task Requirements:**\n",
    "\n",
    "**1.Define Model** (5 Pts)  \n",
    "    - Correctly define the RNN model structure, including the embedding layer, RNN layer, and fully connected layer.\n",
    "\n",
    "**2.Define Loss Function and Optimizer** (3 Pts)  \n",
    "    - Set up the loss function as `BCELoss` and configure the optimizer as `Adam` with an appropriate learning rate.\n",
    "\n",
    "**3.Training Code and Output Loss** (6 Pts)  \n",
    "    - Implement the training loop, including forward propagation, loss calculation, backpropagation, and parameter updates.  \n",
    "    - Output the training loss for each epoch.\n",
    "\n",
    "**4.Evaluation Code and Output Loss and Accuracy** (5 Pts)  \n",
    "    - Correctly implement the evaluation process, calculating validation loss and accuracy.  \n",
    "    - Output the validation loss and accuracy for each epoch.\n",
    "\n",
    "**5.Record Historical Loss and Plot Trend Line** (3 Pts)  \n",
    "    - Record the loss for each epoch during training.  \n",
    "    - Plot a line chart of training loss to show the trend.\n",
    "\n",
    "**6.Record Historical Accuracy and Plot Trend Line** (3 Pts)  \n",
    "    - Record the validation accuracy for each epoch.  \n",
    "    - Plot a line chart of accuracy changes to show the trend in model accuracy on the validation set.\n",
    "\n",
    "**7.Evaluate Accuracy and Optimize Hyperparameters** (5 Pts)  \n",
    "   - After completing the model training, make every effort to improve the model's accuracy by tuning hyperparameters. Your score will be awarded based on the achieved accuracy according to the following criteria:\n",
    "     - [60%,80%) accuracy: 1 points\n",
    "     - [80%,90%) accuracy: 2 points\n",
    "     - [90%,95%) accuracy: 3 points\n",
    "     - [90%,95%) accuracy: 4 points\n",
    "     -  95%+ accuracy: 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76bbff-dc0b-4cee-b6cf-e3a85b69a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- Complete your answer here ---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65741fb2-5d5a-49f7-bdea-7e24bb6d034b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284ffb2-5117-4b8e-a9dd-a770e0900d85",
   "metadata": {},
   "source": [
    "We can try predicting some custom sentences to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481b0cb-6858-405a-9e5e-e64400aaa209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for sentiment prediction\n",
    "def predict_sentiment(sentence, model, vocab, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Tokenize the sentence and convert tokens to indices\n",
    "    tokens = [vocab.get(token, vocab[\"<unk>\"]) for token in jieba.cut(sentence)]\n",
    "    text_tensor = torch.tensor(tokens, dtype=torch.int64).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    length_tensor = torch.tensor([len(tokens)]).to(device)  # Create a length tensor\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        prediction = model(text_tensor, length_tensor).item()\n",
    "\n",
    "    # Output the prediction result\n",
    "    return \"pos\" if prediction > 0.5 else \"neg\"\n",
    "\n",
    "# Example usage\n",
    "test_sentence = \"这部电影非常精彩，值得一看！\"  # Test sentence\n",
    "result = predict_sentiment(test_sentence, model, vocab, device)\n",
    "print(f\"Sentiment classification result for '{test_sentence}': {result}\")\n",
    "\n",
    "test_sentence = \"太差了吧！！！\"  # Another test sentence\n",
    "result = predict_sentiment(test_sentence, model, vocab, device)\n",
    "print(f\"Sentiment classification result for '{test_sentence}': {result}\")\n",
    "\n",
    "test_sentence = \"\"\"北京师范大学-香港浸会大学联合国际学院（中文简称“北师港浸大”，英文简称“UIC”）成立于2005年，\n",
    "是首家中国内地与香港高等教育界合作创办的大学，也是内地第一所创新的博雅型大学，位于粤港澳大湾区的珠海市。\n",
    "秉承博雅教育办学理念，北师港浸大致力于为内地高等教育多元化发展开辟一条新路，为国家和社会培养专业知识与综合素质双修、\n",
    "家国情怀与国际视野兼具的新时代学子。\"\"\"  # Long text for testing\n",
    "result = predict_sentiment(test_sentence, model, vocab, device)\n",
    "print(f\"Sentiment classification result for '{test_sentence}': {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49666d-d882-4082-8289-39a505d9d871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
